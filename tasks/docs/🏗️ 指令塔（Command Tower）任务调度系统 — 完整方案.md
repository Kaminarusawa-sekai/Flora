

------

# ğŸ—ï¸ æŒ‡ä»¤å¡”ï¼ˆCommand Towerï¼‰ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ â€” å®Œæ•´æ–¹æ¡ˆ

> **ç›®æ ‡**ï¼šæ”¯æŒæ™ºèƒ½ä½“å·¥ä½œæµï¼ˆAgent Workflowï¼‰ï¼Œå…·å¤‡ï¼š
>
> - é€’å½’è£‚å˜ï¼ˆLayered Executionï¼‰
> - DAG ä¾èµ–è°ƒåº¦ï¼ˆdepends_onï¼‰
> - åŸç”Ÿ CRONï¼ˆå¤š traceï¼‰ä¸ LOOPï¼ˆå• trace å¤šè½®ï¼‰
> - å…¨å±€ä¿¡å·æ§åˆ¶ï¼ˆæŒ‰ trace_id å–æ¶ˆï¼‰
> - é«˜æ•ˆçŠ¶æ€æŸ¥è¯¢ä¸è¿›åº¦ä¸ŠæŠ¥

------

## ä¸€ã€é¡¹ç›®ç»“æ„ï¼ˆPython + Async + Pydanticï¼‰

```
command_tower/

â”œâ”€â”€ common/                # L1: é¢†åŸŸæ¨¡å‹ï¼ˆçº¯ DTOï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ enums.py
â”‚   â”œâ”€â”€ task_definition.py
â”‚   â””â”€â”€ task_instance.py
â”‚
â”œâ”€â”€ external/                 # L2: åŸºç¡€è®¾æ–½æŠ½è±¡ä¸å®ç°
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ models.py       # SQLAlchemy ORM
â”‚   â”‚   â””â”€â”€ sqlalchemy_impl.py
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â””â”€â”€ redis_impl.py
â”‚   â””â”€â”€ messaging/
â”‚      â”œâ”€â”€ __init__.py
â”‚      â”œâ”€â”€ base.py
â”‚      â””â”€â”€ rabbitmq_delayed.py
â”‚   â”‚
â”œâ”€â”€ services/           # L3: åº”ç”¨æœåŠ¡ï¼ˆUse Caseï¼‰
â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lifecycle_service.py
â”‚   â”œâ”€â”€ signal_service.py
â”‚   â””â”€â”€ observer_service.py
â”‚
â”œâ”€â”€ drivers/               # L4: é©±åŠ¨å±‚ï¼ˆå…¥å£ï¼‰
â”‚   â”œâ”€â”€ schedulers/
â”‚   â”‚   â”œâ”€â”€ cron_generator.py
â”‚   â”‚   â”œâ”€â”€ dispatcher.py
â”‚   â”‚   â””â”€â”€ loop_controller.py  # å¯é€‰ï¼Œç”± LifecycleService å†…éƒ¨å¤„ç†
â”‚   
â”œâ”€â”€ apps/ 
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ commands.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ queries.py
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ worker_client.py    # æ¨¡æ‹Ÿ Worker ä¸ŠæŠ¥
â”‚   â”‚
â””â”€â”€ main.py                # å¯åŠ¨å…¥å£
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # é…ç½®ç®¡ç†
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ docker-compose.yml         # å« RabbitMQï¼ˆå¸¦å»¶æ—¶æ’ä»¶ï¼‰ã€Redisã€PostgreSQL
```

------

## äºŒã€L1ï¼šé¢†åŸŸæ¨¡å‹ï¼ˆDomain Modelsï¼‰

### `common/enums.py`

```python
from enum import Enum

class ActorType(str, Enum):
    AGENT = "AGENT"
    GROUP_AGG = "GROUP_AGG"
    SINGLE_AGG = "SINGLE_AGG"
    EXECUTION = "EXECUTION"

class ScheduleType(str, Enum):
    ONCE = "ONCE"
    CRON = "CRON"
    LOOP = "LOOP"

class TaskInstanceStatus(str, Enum):
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    SUCCESS = "SUCCESS"
    FAILED = "FAILED"
    CANCELLED = "CANCELLED"
    SKIPPED = "SKIPPED"
```

### `common/task_definition.py`

```python
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from .enums import ActorType, ScheduleType

class TaskDefinition(BaseModel):
    id: str = Field(..., description="ä»»åŠ¡å®šä¹‰å”¯ä¸€ID")
    name: str
    actor_type: ActorType
    role: Optional[str] = None
    code_ref: str = Field(..., description="å¦‚ docker://my/agent:v1")
    entrypoint: str = "main.run"

    schedule_type: ScheduleType = ScheduleType.ONCE
    cron_expr: Optional[str] = None
    loop_config: Optional[Dict[str, Any]] = None

    resource_profile: str = "default"
    strategy_tags: List[str] = Field(default_factory=list)

    default_params: Dict[str, Any] = Field(default_factory=dict)
    timeout_sec: int = 300
    max_retries: int = 3
    is_active: bool = True
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

### `common/task_instance.py`

```python
from datetime import datetime
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from .enums import ActorType, ScheduleType, TaskInstanceStatus

class TaskInstance(BaseModel):
    id: str
    trace_id: str
    parent_id: Optional[str] = None
    job_id: str

    actor_type: ActorType
    role: Optional[str] = None
    layer: int = 0
    is_leaf_agent: bool = False

    schedule_type: ScheduleType = ScheduleType.ONCE
    round_index: Optional[int] = None
    cron_trigger_time: Optional[datetime] = None

    status: TaskInstanceStatus
    node_path: str
    depth: int = 0
    depends_on: Optional[List[str]] = None

    split_count: int = 0
    completed_children: int = 0

    input_params: Dict[str, Any] = Field(default_factory=dict)
    output_ref: Optional[str] = None
    error_msg: Optional[str] = None

    started_at: Optional[datetime] = None
    finished_at: Optional[datetime] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
```

------

## ä¸‰ã€L2ï¼šåŸºç¡€è®¾æ–½å±‚ï¼ˆInfraï¼‰

### `external/db/base.py`

```python
from abc import ABC
from typing import List, Optional
from app.domain.task_instance import TaskInstance
from app.domain.task_definition import TaskDefinition

class TaskDefinitionRepository(ABC):
    async def get(self, def_id: str) -> TaskDefinition: ...
    async def list_active_cron(self) -> List[TaskDefinition]: ...

class TaskInstanceRepository(ABC):
    async def create(self, instance: TaskInstance) -> None: ...
    async def get(self, instance_id: str) -> TaskInstance: ...
    async def get_by_ids(self, ids: List[str]) -> List[TaskInstance]: ...
    async def find_by_trace_id(self, trace_id: str) -> List[TaskInstance]: ...
    async def lock_for_execution(self, instance_id: str, worker_id: str) -> bool: ...
    async def update_status(self, instance_id: str, status: TaskInstanceStatus, **kwargs) -> None: ...
    async def increment_completed_children(self, parent_id: str) -> int: ...
    async def find_ready_tasks(self) -> List[TaskInstance]: ...
    async def bulk_update_status_by_trace(self, trace_id: str, status: TaskInstanceStatus) -> None: ...
```

### `external/db/models.py`ï¼ˆSQLAlchemyï¼‰

```python
from sqlalchemy import Column, String, Integer, Boolean, DateTime, Text, JSON, ForeignKey, Index
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func

Base = declarative_base()

class TaskInstanceDB(Base):
    __tablename__ = "task_instances"

    id = Column(String, primary_key=True)
    trace_id = Column(String, index=True)
    parent_id = Column(String, index=True)
    job_id = Column(String)

    actor_type = Column(String)
    layer = Column(Integer)
    is_leaf_agent = Column(Boolean)

    schedule_type = Column(String)
    round_index = Column(Integer, nullable=True)
    cron_trigger_time = Column(DateTime, nullable=True)

    status = Column(String, index=True)
    node_path = Column(String)
    depth = Column(Integer)
    depends_on = Column(JSON)

    split_count = Column(Integer, default=0)
    completed_children = Column(Integer, default=0)

    input_params = Column(JSON)
    output_ref = Column(String, nullable=True)
    error_msg = Column(Text, nullable=True)

    started_at = Column(DateTime, nullable=True)
    finished_at = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=func.now())
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())

Index("idx_status_pending", TaskInstanceDB.status, postgresql_where=(TaskInstanceDB.status == 'PENDING'))
```

### `external/db/sqlalchemy_impl.py`ï¼ˆå…³é”®æ–¹æ³•ï¼‰

```python
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update, and_
from .base import TaskInstanceRepository
from .models import TaskInstanceDB
from app.domain.task_instance import TaskInstance
from app.domain.enums import TaskInstanceStatus
import json

class SQLAlchemyTaskInstanceRepo(TaskInstanceRepository):
    def __init__(self, session: AsyncSession):
        self.session = session

    async def find_ready_tasks(self) -> List[TaskInstance]:
        # æŸ¥æ‰¾æ‰€æœ‰ PENDING ä¸”æ— ä¾èµ– or ä¾èµ–å·²å…¨éƒ¨å®Œæˆçš„ä»»åŠ¡
        stmt = select(TaskInstanceDB).where(
            TaskInstanceDB.status == TaskInstanceStatus.PENDING
        )
        result = await self.session.execute(stmt)
        candidates = result.scalars().all()

        ready = []
        for t in candidates:
            if not t.depends_on:
                ready.append(t)
            else:
                dep_ids = t.depends_on
                dep_stmt = select(TaskInstanceDB.status).where(
                    TaskInstanceDB.id.in_(dep_ids)
                )
                deps = (await self.session.execute(dep_stmt)).scalars().all()
                if all(s == TaskInstanceStatus.SUCCESS for s in deps):
                    ready.append(t)
        return [self._to_domain(t) for t in ready]

    async def increment_completed_children(self, parent_id: str) -> int:
        stmt = (
            update(TaskInstanceDB)
            .where(TaskInstanceDB.id == parent_id)
            .values(completed_children=TaskInstanceDB.completed_children + 1)
            .returning(TaskInstanceDB.completed_children)
        )
        result = await self.session.execute(stmt)
        await self.session.commit()
        return result.scalar_one()

    def _to_domain(self, db: TaskInstanceDB) -> TaskInstance:
        return TaskInstance(
            id=db.id,
            trace_id=db.trace_id,
            parent_id=db.parent_id,
            job_id=db.job_id,
            actor_type=db.actor_type,
            layer=db.layer,
            is_leaf_agent=db.is_leaf_agent,
            schedule_type=db.schedule_type,
            round_index=db.round_index,
            cron_trigger_time=db.cron_trigger_time,
            status=TaskInstanceStatus(db.status),
            node_path=db.node_path,
            depth=db.depth,
            depends_on=json.loads(db.depends_on) if db.depends_on else None,
            split_count=db.split_count,
            completed_children=db.completed_children,
            input_params=json.loads(db.input_params) if db.input_params else {},
            output_ref=db.output_ref,
            error_msg=db.error_msg,
            started_at=db.started_at,
            finished_at=db.finished_at,
            created_at=db.created_at,
            updated_at=db.updated_at
        )
```

> ğŸ’¡ **æ³¨æ„**ï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ç¼“å­˜ä¼˜åŒ– `depends_on` éªŒè¯ï¼ˆå¦‚ Redis å­˜æ¯ä¸ª task çš„å®ŒæˆçŠ¶æ€ï¼‰ã€‚

------

## å››ã€L3ï¼šåº”ç”¨æœåŠ¡å±‚ï¼ˆApplication Servicesï¼‰

### `services/lifecycle_service.py`

```python
import uuid
from datetime import datetime
from app.domain.task_instance import TaskInstance
from app.domain.enums import TaskInstanceStatus, ScheduleType
from app.infra.db.base import TaskDefinitionRepository, TaskInstanceRepository
from app.infra.messaging.base import MessageBroker

class LifecycleService:
    def __init__(
        self,
        def_repo: TaskDefinitionRepository,
        inst_repo: TaskInstanceRepository,
        broker: MessageBroker
    ):
        self.def_repo = def_repo
        self.inst_repo = inst_repo
        self.broker = broker

    async def start_new_trace(
        self,
        def_id: str,
        input_params: dict,
        trigger_type: str = "MANUAL"
    ) -> str:
        definition = await self.def_repo.get(def_id)
        trace_id = str(uuid.uuid4())
        job_id = f"job-{trace_id[:8]}"
        root_id = str(uuid.uuid4())

        root = TaskInstance(
            id=root_id,
            trace_id=trace_id,
            job_id=job_id,
            parent_id=None,
            actor_type=definition.actor_type,
            role=definition.role,
            layer=0,
            is_leaf_agent=(definition.actor_type == "AGENT" and not definition.role),  # ç®€åŒ–åˆ¤æ–­
            schedule_type=definition.schedule_type,
            round_index=0 if definition.schedule_type == ScheduleType.LOOP else None,
            cron_trigger_time=datetime.utcnow() if definition.schedule_type == ScheduleType.CRON else None,
            status=TaskInstanceStatus.PENDING,
            node_path="/",
            input_params={**definition.default_params, **input_params},
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        await self.inst_repo.create(root)
        await self._schedule_task(root)
        return trace_id

    async def handle_task_completed(self, task_id: str, output_ref: str):
        task = await self.inst_repo.get(task_id)
        await self.inst_repo.update_status(task_id, TaskInstanceStatus.SUCCESS, output_ref=output_ref, finished_at=datetime.utcnow())

        # é€šçŸ¥çˆ¶èŠ‚ç‚¹
        if task.parent_id:
            new_count = await self.inst_repo.increment_completed_children(task.parent_id)
            parent = await self.inst_repo.get(task.parent_id)
            if new_count >= parent.split_count:
                await self._activate_aggregator(parent)

        # LOOP ä¸‹ä¸€è½®
        if task.schedule_type == ScheduleType.LOOP:
            definition = await self.def_repo.get(task.definition_id)
            max_rounds = definition.loop_config.get("max_rounds", 1)
            current_round = task.round_index or 0
            if current_round + 1 < max_rounds:
                interval = definition.loop_config.get("interval_sec", 10)
                await self.inst_repo.update_status(
                    task_id,
                    TaskInstanceStatus.PENDING,
                    round_index=current_round + 1,
                    started_at=None,
                    finished_at=None,
                    updated_at=datetime.utcnow()
                )
                await self.broker.publish_delayed(
                    "task.execute",
                    {"instance_id": task_id},
                    delay_sec=interval
                )

    async def _activate_aggregator(self, parent: TaskInstance):
        await self.inst_repo.update_status(parent.id, TaskInstanceStatus.RUNNING, started_at=datetime.utcnow())
        await self.broker.publish("task.execute", {"instance_id": parent.id})

    async def _schedule_task(self, task: TaskInstance):
        # å³ä½¿ delay=0 ä¹Ÿèµ°å»¶æ—¶é˜Ÿåˆ—ï¼Œç»Ÿä¸€å…¥å£
        await self.broker.publish_delayed("task.execute", {"instance_id": task.id}, delay_sec=0)
```

### `services/signal_service.py`

```python
from app.infra.cache.base import CacheClient
from app.infra.db.base import TaskInstanceRepository
from app.domain.enums import TaskInstanceStatus

class SignalService:
    def __init__(self, cache: CacheClient, inst_repo: TaskInstanceRepository):
        self.cache = cache
        self.inst_repo = inst_repo

    async def cancel_trace(self, trace_id: str):
        await self.cache.set(f"trace_signal:{trace_id}", "CANCEL", ttl=3600)
        await self.inst_repo.bulk_update_status_by_trace(trace_id, TaskInstanceStatus.CANCELLED)
```

------

## äº”ã€L4ï¼šé©±åŠ¨å±‚ï¼ˆDriversï¼‰

### `drivers/schedulers/dispatcher.py`

```python
import asyncio
from app.infra.messaging.base import MessageBroker
from app.infra.db.base import TaskInstanceRepository
from app.application.lifecycle_service import LifecycleService

async def task_execute_consumer(broker: MessageBroker, inst_repo: TaskInstanceRepository, worker_url: str):
    async def handler(msg: dict):
        task_id = msg["instance_id"]
        task = await inst_repo.get(task_id)

        # æ£€æŸ¥ trace æ˜¯å¦è¢«å–æ¶ˆ
        from app.infra.cache.redis_impl import redis_client
        signal = await redis_client.get(f"trace_signal:{task.trace_id}")
        if signal == "CANCEL":
            return

        # æ£€æŸ¥ä¾èµ–ï¼ˆDAGï¼‰
        if task.depends_on:
            deps = await inst_repo.get_by_ids(task.depends_on)
            if any(d.status != "SUCCESS" for d in deps):
                await broker.publish_delayed("task.execute", msg, 5)
                return

        # æŠ¢é”æ´¾å‘
        if await inst_repo.lock_for_execution(task_id, "worker-01"):
            # æ¨¡æ‹Ÿè°ƒç”¨ Worker
            import httpx
            async with httpx.AsyncClient() as client:
                await client.post(f"{worker_url}/execute", json=task.dict())

    await broker.consume("task.execute", handler)
```

### `apps/api/v1/commands.py`

```python
from fastapi import APIRouter, Body
from app.application.lifecycle_service import LifecycleService
from app.application.signal_service import SignalService

router = APIRouter()

@router.post("/traces/start")
async def start_trace(def_id: str = Body(...), params: dict = Body({})):
    trace_id = await lifecycle_svc.start_new_trace(def_id, params)
    return {"trace_id": trace_id}

@router.post("/traces/{trace_id}/cancel")
async def cancel_trace(trace_id: str):
    await signal_svc.cancel_trace(trace_id)
    return {"status": "cancelled"}
```

### `apps/api/v1/queries.py`

```python
@router.get("/traces/{trace_id}/tasks")
async def get_trace_tasks(trace_id: str, status: str = None, layer: int = None):
    tasks = await inst_repo.find_by_trace_id(trace_id)
    filtered = tasks
    if status: filtered = [t for t in filtered if t.status.value == status]
    if layer is not None: filtered = [t for t in filtered if t.layer == layer]
    return filtered
```

------

## å…­ã€ä¸­é—´ä»¶é…ç½®

### `docker-compose.yml`

```yaml
version: '3.8'
services:
  rabbitmq:
    image: rabbitmq:3.12-management
    ports: ["5672:5672", "15672:15672"]
    environment:
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: "-rabbitmq_delayed_message_exchange true"
    volumes:
      - ./rabbitmq-delayed-plugin:/opt/rabbitmq/plugins/rabbitmq_delayed_message_exchange

  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: command_tower
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports: ["5432:5432"]
    volumes: ["pgdata:/var/lib/postgresql/data"]

volumes:
  pgdata:
```

> âš ï¸ **RabbitMQ å»¶æ—¶æ’ä»¶**éœ€æå‰ä¸‹è½½ `.ez` æ–‡ä»¶æ”¾å…¥æŒ‚è½½ç›®å½•ã€‚

------

## ä¸ƒã€Worker æ‰§è¡Œåè®®ï¼ˆç®€ç‰ˆï¼‰

Worker æ”¶åˆ°ä»»åŠ¡åï¼š

1. æ£€æŸ¥ `trace_signal:{trace_id}` æ˜¯å¦ä¸º CANCEL

2. æ‰§è¡Œä¸šåŠ¡é€»è¾‘

3. è‹¥æ˜¯ AGENT ä¸”éå¶å­ï¼Œç”Ÿæˆå­ä»»åŠ¡ï¼ˆè°ƒç”¨ API `/traces/{trace_id}/split`ï¼‰

4. å®Œæˆåä¸ŠæŠ¥ï¼š

   ```http
   POST /events
   {
     "event_type": "COMPLETED",
     "task_id": "...",
     "output_ref": "s3://result/..."
   }
   ```

------

## å…«ã€æ€»ç»“ï¼šæ¶æ„èƒ½åŠ›çŸ©é˜µ

| èƒ½åŠ›           | å®ç°æ–¹å¼                                              |
| -------------- | ----------------------------------------------------- |
| **é€’å½’è£‚å˜**   | `layer` + `is_leaf_agent` æ§åˆ¶æ˜¯å¦ç”Ÿæˆå­ä»»åŠ¡          |
| **DAG è°ƒåº¦**   | `depends_on` + `find_ready_tasks` ä¾èµ–è§£æ            |
| **CRON ä»»åŠ¡**  | æ¯æ¬¡è§¦å‘æ–°å»º `trace_id`ï¼Œç‹¬ç«‹ç”Ÿå‘½å‘¨æœŸ                 |
| **LOOP ä»»åŠ¡**  | å• `trace_id` å†… `round_index` é€’å¢ï¼Œå»¶æ—¶æ¶ˆæ¯æ§åˆ¶     |
| **å…¨å±€å–æ¶ˆ**   | Redis ä¿¡å· `trace_signal:{trace_id}`ï¼ŒWorker ä¸»åŠ¨æ£€æŸ¥ |
| **èšåˆè§¦å‘**   | `split_count` vs `completed_children` è®¡æ•°å™¨          |
| **é«˜æ•ˆæŸ¥è¯¢**   | DB ç´¢å¼•ï¼ˆ`trace_id`, `status`ï¼‰+ åˆ†é¡µ                 |
| **é«˜å¹¶å‘ä¸ŠæŠ¥** | è¿›åº¦å†™ Redisï¼Œæœ€ç»ˆæ€å¼‚æ­¥æŒä¹…åŒ–                        |

------

è¿™ä»½æ–¹æ¡ˆå¯ç›´æ¥ç”¨äºå·¥ç¨‹è½åœ°ã€‚å¦‚éœ€ï¼Œæˆ‘å¯æä¾›ï¼š

- å®Œæ•´ `requirements.txt`
- RabbitMQ å»¶æ—¶æ’ä»¶å®‰è£…è„šæœ¬
- åˆå§‹åŒ–æ•°æ®åº“è¡¨çš„ Alembic è„šæœ¬
- Worker æ¨¡æ‹Ÿå™¨ä»£ç 

è¯·å‘Šè¯‰æˆ‘ä¸‹ä¸€æ­¥éœ€è¦ä»€ä¹ˆï¼Ÿ